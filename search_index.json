[
["index.html", "Statistics: A Reference About", " Statistics: A Reference https://github.com/statistics-reference/ 2020-10-04 About This “book” is designed to be a reference for statistics and data science. It will attempt to present information where the most relevant information appears first. The goal is to cut down noise and to get you relevant information as quickly as possible. I hope this sets this guide apart from others. The book is currently divided into three sections: Descriptive Techniques: Visualization, Exploratory Data Analysis, Model Diagnostics, Etc… Inferential Techniques: Estimation, Models, Prediction, Etc… Computational Techniques: Simulation, Approximation, Algorithms, Etc… "],
["descriptive.html", "1 Overview", " 1 Overview Part I of the book covers what is most commonly considered to be descriptive statistics. There are currently three sections. Graphs And Visualization: This covers how to visually display our data. Common Statistics And Measures: This covers common statistics that can be used to describe the data. Distributions: This covers the most common distributions in statistics Exploratory Data Analysis: Need to figure out where to put data cleaning, data transformatoins, diagnostics Model Checking: This may go here or within statistical inference section. "],
["descriptive-visualization.html", "2 Overview", " 2 Overview Topics to be covered here include: Data visualization "],
["what-graph-should-i-use.html", "3 What Graph Should I Use?", " 3 What Graph Should I Use? "],
["common-graphs-a-n.html", "4 Common Graphs (A to N) 4.1 Area Chart 4.2 Bar Chart 4.3 Bubble Chart", " 4 Common Graphs (A to N) This section covers covers common graphs and visualization techniques for examining your sample data. Visualization Techniques: Bar Charts: Histograms: 4.1 Area Chart 4.1.1 Overview and Summary 4.1.2 Common Applications 4.1.3 Interpretation 4.1.4 Doing It In R Coming soon 4.1.5 Sources and Useful Links Sources: Useful Links: 4.2 Bar Chart An example of a bar chart is shown below: 4.2.1 Overview and Summary Bar charts are used to compare categorical variables. Usually on one axis is the categorical variable and on the other axis is a quantitative measure of that variable such as a count or the mean. Bar charts with the categorical variable on the x-axis are sometimes called column charts while bar charts with categorical variables on the y-axis are sometimes called row charts. In the above, we see a column chart with the categorical variable on the x-axis. Bar charts are very similar to histograms except that typically histograms are used to depict quantitative variables as opposed to categorical variables. 4.2.2 Common Applications Bar charts are useful to summarize and compare groups (or levels) of a categorical variable. 4.2.3 Interpretation Generally, the height of a bar corresponds to the value of the quantitative measure. Do be careful during interpretation to examine the range on the axis for the quantitative measure. It is possible that the axis does not start at zero, thus leading to deceiving bar heights. This is seen below. In the below, we see two charts that depict the exact same data and information. On the left, the bar chart has a y-axis ranging from 0 to 30. Note that the values are actually pretty similar to each other. On the right, we changed the range of the y-axis to be between 10 and 30. Note how visually, now it seems that the values are now much further from each other. The middle bar looks like it is less than 4 times the height of other bars. Typically, practitioners do not adjust the y-axis to distort bar heights. 4.2.4 Doing It In R In the below, we create bar charts where the categorical variable is planets using R. The below depict the number of hours in a day for different planets. Data is pulled on 9/27/2020 from NASA. 4.2.4.1 Bar Chart In Base R Barcharts in base R can be created with the barplot() function. The below is an example with code. hours = c(24, 24.7, 10.7, 17.2, 16.1) planets = c(&quot;Earth&quot;, &quot;Mars&quot;, &quot;Saturn&quot;, &quot;Uranus&quot;, &quot;Neptune&quot; ) display_color = c(&quot;#EB984E&quot;, &quot;#1ABC9C&quot;, &quot;#95A5A6&quot;, &quot;#3498DB&quot;, &quot;#F1C40F&quot;) barplot(height = hours, # vector of numerical data space = 1, # changes the space between each bar names.arg = planets, # adds labels for the bars horiz = FALSE, # FALSE means bars are drawn vertically col = display_color, # vector of colors for your bars xlab = &quot;Planets&quot;, # label for x-axis ylab = &quot;Hours In A Day&quot;, # label for y-axis main = &quot;How Long Are Days On Planets?&quot;, # title of chart ylim = c(0, 30), # range for the y-axis border = NA) # adjusts the borders of your bars 4.2.4.2 Bar Chart In ggplot2 Within the package ggplot2, barcharts can be created with the geom_bar() or geom_col() function. The below is an example with code. library(ggplot2) # import ggplot2 df = data.frame(&quot;Planets&quot; = c(&quot;Earth&quot;, &quot;Mars&quot;, &quot;Saturn&quot;, &quot;Uranus&quot;, &quot;Neptune&quot; ), &quot;Hours In A Day&quot; = c(24, 24.7, 10.7, 17.2, 16.1), check.names = FALSE) # create dataframe of data display_color = c(&quot;#EB984E&quot;, &quot;#1ABC9C&quot;, &quot;#95A5A6&quot;, &quot;#3498DB&quot;, &quot;#F1C40F&quot;) # colors for bars g = ggplot(df, aes_string(x = &quot;Planets&quot;, # data on x-axis y = &quot;`Hours In A Day`&quot;)) # data on y-axis g = g + geom_col(fill = display_color, # change the color of the bar width = 0.7) # change width of bar g = g + ggtitle(&quot;How Long Are Days On Planets?&quot;) # add title g = g + theme(panel.background = element_rect(fill=&#39;#FFFFFF&#39;, colour=&#39;#95A5A6&#39;)) # edit background color g = g + scale_y_continuous(expand = expansion(mult = c(0, .1)), # stretch plot to fit grid breaks = round(seq(0, 30, by = 5),1)) # change interval for y-axis g = g + theme(axis.line = element_line(colour = &quot;#95A5A6&quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_blank(), panel.background = element_blank(), text = element_text(size=12, family = &quot;sans&quot;)) # change font g 4.2.5 Sources and Useful Links Sources: https://en.wikipedia.org/wiki/Bar_chart Useful Links: https://ggplot2.tidyverse.org/reference/geom_bar.html http://r-statistics.co/Complete-Ggplot2-Tutorial-Part1-With-R-Code.html https://rstudio.com/wp-content/uploads/2016/11/ggplot2-cheatsheet-2.1.pdf https://stackoverflow.com/questions/20220424/ggplot2-bar-plot-no-space-between-bottom-of-geom-and-x-axis-keep-space-above/50697152 https://stackoverflow.com/questions/32941670/width-and-gap-of-geom-bar-ggplot2 https://stackoverflow.com/questions/10861773/remove-grid-background-color-and-top-and-right-borders-from-ggplot2 4.3 Bubble Chart An example of a bubble chart is shown below: 4.3.1 Overview and Summary Bubble charts are useful to depict three dimensional data. That is, when y 4.3.2 Common Applications Bar charts are useful to summarize and compare groups (or levels) of a categorical variable. 4.3.3 Interpretation Generally, the height of a bar corresponds to the value of the quantitative measure. Do be careful during interpretation to examine the range on the axis for the quantitative measure. It is possible that the axis does not start at zero, thus leading to deceiving bar heights. This is seen below. In the below, we see two charts that depict the exact same data and information. On the left, the bar chart has a y-axis ranging from 0 to 30. Note that the values are actually pretty similar to each other. On the right, we changed the range of the y-axis to be between 10 and 30. Note how visually, now it seems that the values are now much further from each other. The middle bar looks like it is less than 4 times the height of other bars. Typically, practitioners do not adjust the y-axis to distort bar heights. 4.3.4 Doing It In R In the below, we create bar charts where the categorical variable is planets using R. The below depict the number of hours in a day for different planets. Data is pulled on 9/27/2020 from NASA. 4.3.4.1 Bubble Chart In Base R Barcharts in base R can be created with the barplot() function. The below is an example with code. hours = c(24, 24.7, 10.7, 17.2, 16.1) planets = c(&quot;Earth&quot;, &quot;Mars&quot;, &quot;Saturn&quot;, &quot;Uranus&quot;, &quot;Neptune&quot; ) display_color = c(&quot;#EB984E&quot;, &quot;#1ABC9C&quot;, &quot;#95A5A6&quot;, &quot;#3498DB&quot;, &quot;#F1C40F&quot;) barplot(height = hours, # vector of numerical data space = 1, # changes the space between each bar names.arg = planets, # adds labels for the bars horiz = FALSE, # FALSE means bars are drawn vertically col = display_color, # vector of colors for your bars xlab = &quot;Planets&quot;, # label for x-axis ylab = &quot;Hours In A Day&quot;, # label for y-axis main = &quot;How Long Are Days On Planets?&quot;, # title of chart ylim = c(0, 30), # range for the y-axis border = NA) # adjusts the borders of your bars 4.3.4.2 Bubble Chart In ggplot2 Within the package ggplot2, barcharts can be created with the geom_bar() or geom_col() function. The below is an example with code. library(ggplot2) # import ggplot2 df = data.frame(&quot;Planets&quot; = c(&quot;Earth&quot;, &quot;Mars&quot;, &quot;Saturn&quot;, &quot;Uranus&quot;, &quot;Neptune&quot; ), &quot;Hours In A Day&quot; = c(24, 24.7, 10.7, 17.2, 16.1), check.names = FALSE) # create dataframe of data display_color = c(&quot;#EB984E&quot;, &quot;#1ABC9C&quot;, &quot;#95A5A6&quot;, &quot;#3498DB&quot;, &quot;#F1C40F&quot;) # colors for bars g = ggplot(df, aes_string(x = &quot;Planets&quot;, # data on x-axis y = &quot;`Hours In A Day`&quot;)) # data on y-axis g = g + geom_col(fill = display_color, # change the color of the bar width = 0.7) # change width of bar g = g + ggtitle(&quot;How Long Are Days On Planets?&quot;) # add title g = g + theme(panel.background = element_rect(fill=&#39;#FFFFFF&#39;, colour=&#39;#95A5A6&#39;)) # edit background color g = g + scale_y_continuous(expand = expansion(mult = c(0, .1)), # stretch plot to fit grid breaks = round(seq(0, 30, by = 5),1)) # change interval for y-axis g = g + theme(axis.line = element_line(colour = &quot;#95A5A6&quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_blank(), panel.background = element_blank(), text = element_text(size=12, family = &quot;sans&quot;)) # change font g 4.3.5 Sources and Useful Links Sources: https://en.wikipedia.org/wiki/Bar_chart Useful Links: https://ggplot2.tidyverse.org/reference/geom_bar.html http://r-statistics.co/Complete-Ggplot2-Tutorial-Part1-With-R-Code.html https://rstudio.com/wp-content/uploads/2016/11/ggplot2-cheatsheet-2.1.pdf https://stackoverflow.com/questions/20220424/ggplot2-bar-plot-no-space-between-bottom-of-geom-and-x-axis-keep-space-above/50697152 https://stackoverflow.com/questions/32941670/width-and-gap-of-geom-bar-ggplot2 https://stackoverflow.com/questions/10861773/remove-grid-background-color-and-top-and-right-borders-from-ggplot2 "],
["common-graphs-o-z.html", "5 Common Graphs (O to Z) 5.1 Pie Chart", " 5 Common Graphs (O to Z) This section covers common graphs and visualization techniques for examining and summarizing your data. Graphs range from O to Z. Visualization Techniques: Pie Charts: 5.1 Pie Chart An example of a pie chart is shown below: 5.1.1 Overview and Summary Pie charts are used to display proportions or percentages. Each “slice” of a pie represents a different category and will have an associated percentage. 5.1.2 Common Applications Pie charts are useful to summarize and compare categories in percentage or relative terms. 5.1.3 Interpretation Generally, the size of a slice corresponds to the proportion of the category relative to others. While pie charts are useful at a glance, it can be difficult to determine exactly how much larger or smaller one category is. 5.1.4 Doing It In R In the below, we create pie charts that break down the GDP of Mauritania by sector. Data is pulled on 10/04/2020 from Wikipedia. 5.1.4.1 Pie Chart In Base R Pie charts in base R can be created with the pie() function. The below is an example with code. To build one, you need a vector of values and a vector of labels. The vector of values do NOT have to be in percent form. They can be the original values - R will automatically scale them within the pie chart. values = c(14.9, 48.0, 37.1) sectors = c(&quot;Agriculture&quot;, &quot;Industry&quot;, &quot;Services&quot;) display_color = c(&quot;#EB984E&quot;, &quot;#1ABC9C&quot;, &quot;#95A5A6&quot;) pie(x = values, # vector of data labels = paste0(sectors, &quot; - &quot;, round(100 * values/sum(values)), &quot;%&quot;), # labels for each slice col = display_color, edges = 20000, # smoothness of your circle radius = 1, # size of pie border = NA, # adjusts the borders of your pie main = &quot;GDP By Sector&quot; # title of chart ) 5.1.4.2 Pie Chart In ggplot2 Within the package ggplot2, pie charts can be created with the function used to create bar charts geom_bar(). We then convert the bar chart to a pie chart using coord_polar(). The below is an example with code. library(ggplot2) # import ggplot2 # Create dataframe of data df = data.frame(&quot;Sectors&quot; = c(&quot;Agriculture&quot;, &quot;Industry&quot;, &quot;Services&quot;), # labels for your data &quot;Values&quot; = c(14.9, 48.0, 37.1)) # values of your data display_color = c(&quot;#EB984E&quot;, &quot;#1ABC9C&quot;, &quot;#95A5A6&quot;) # colors for slices # Create bar plot g = ggplot(df, aes(x = &quot;&quot;, y = Values, # values for your slices fill = Sectors ) ) g = g + geom_bar( # change the color of the slices width = 1, # change width of bar stat = &quot;identity&quot; ) # CONVERT TO PIE CHART g = g + coord_polar(&quot;y&quot;, start = 0) g = g + labs(x = NULL, y = NULL, fill = NULL, title = &quot;GDP By Sector&quot;) g = g + theme(axis.line = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), plot.title = element_text(hjust = 0.5, color = &quot;#000000&quot;), panel.background = element_blank() ) g = g + geom_text(aes(label = scales::percent(values / sum(values))), position = position_stack(vjust = .5) ) # Add custom colors g = g + scale_fill_manual(values = display_color) g 5.1.5 Sources and Useful Links Sources: https://en.wikipedia.org/wiki/Pie_chart Useful Links: "],
["descriptive-common-statisticsl.html", "6 Overview", " 6 Overview Topics to be covered here include: Data Processing, Feature Engineering Performance Evaluation, Model Comparison, Feature Selection, Regularization "],
["descriptive-distributions.html", "7 Overview", " 7 Overview Topics to be covered here include: "],
["inferential.html", "8 Overview", " 8 Overview Part II of the book covers what is most commonly considered to be inferential statistics. There are currently three sections. General Topics: This includes topics related to statistical inference, but is not really a model in itself per say. Models and Methods: This covers actual techniques for drawing conclusions regarding your data. Computation: This covers R and Python code to implement stuff in the first two sections. I am debating on consolidating this section with the above two. "],
["inferential-general.html", "9 Overview", " 9 Overview Topics to be covered here include: Data Processing, Feature Engineering Performance Evaluation, Model Comparison, Feature Selection, Regularization "],
["inferential-models.html", "10 Overview", " 10 Overview Topics to be covered include: Point Inference Statistical Tests Statistical Tests (Time Series) ANOVA and Related Methods Linear Regression (Extensions and Regularization) Logisitc Regression ARIMA Models I am deciding on whether or not to include machine learning methods in this book or not or in another book. This depends on ease of navigation, loading time, and marketing. If so, this would include both supervised and unsupervised methods ranging from KNN, Trees, Ensemble Methods, Neural Networks and more. "],
["inferential-models-point-inference.html", "11 Overview", " 11 Overview "],
["point-inference-mean.html", "12 Mean Inference 12.1 Normal-Univariate-Frequentist-Known-Variance 12.2 Normal-Univariate-Frequentist-Unknown-Variance 12.3 General-Univariate-Frequentist", " 12 Mean Inference This section covers population mean inference (estimating the population mean) under various assumptions and conditions. Each section covers mean inference under different conditions and assumptions. The sections are: Mean Inference (n.1.f.k): normal, iid, univariate, frequentist, known variance Mean Inference (n.1.f): normal, iid, univariate, frequentist, unknown variance Mean Inference (g.1.f): general, iid, univariate, frequentist Mean Inference (n.2+.f): normal, iid, multivariate, frequentist Mean Inference (g.2+.f): general, iid, multivariate, frequentist Mean Inference (n.1.b.k): normal, iid, univariate, Bayesian, known variance Mean Inference (n.1.b): normal, iid, univariate, Bayesian, unknown variance Mean Inference (g.1.b): general, iid, univariate, Bayesian Mean Inference (n.2+.b): normal, iid, multivariate, Bayesian Mean Inference (g.2+.b): general, iid, multivariate, Bayesian 12.1 Normal-Univariate-Frequentist-Known-Variance This section covers population mean inference when the population distribution is assumed to be normal and population variance is known ex ante. More specifically, mean inference is performed under the following assumptions and conditions: Assumptions: Population follows a normal distribution Observations are iid Known population variance ex ante Other Conditions, Criteria, or Attributes: Univariate mean inference Frequentist perspective 12.1.1 Overview and Summary Under the assumptions above, inference can be done using the Z-statistic and Z-distribution (standard normal distribution). Key Takeaways: We calculate the Z-statistic and use the Z-values and Z-distribution (standard normal distribution) to create the confidence interval and perform hypothesis testing since the population variance is known. If the population variance is unknown, we would use the T-statistic and T-distribution. You can think of the test statistic as a way to relate the sample data, our hypothesized population parameter, and a likelihood. Z can be plugged into a standard normal PDF to determine a likelihood. 12.1.2 Point Estimation Most commonly we use the following formula to generate an unbiased estimate of the population mean. The formula is: \\[\\bar{x} = \\frac{\\sum^n_{i=1}{x_{i}}}{n}\\] Where: \\(\\bar{x}\\) = unbiased estimate of population mean \\(n\\) = sample size \\(x_{i}\\) = your sample observations The formula above not only produces an unbiased estimate under the assumptions in this section, but always produces an unbiased estimate of the population mean no matter the distribution (as long as the population mean exists). Key Takeaways: The above formula will produce an unbiased estimate no matter what the population distribution is. While you could use maximum likelihood estimation, method of moments, or other estimation techniques to produce a point estimate for the population mean, in practice we just use the formula above. Under the normal assumption, both maximum likelihood estimation and method of moments will result in the same estimate of the mean as the formula above. 12.1.3 Confidence Interval The formula for the confidence interval given our assumptions is: \\[\\bar{x} \\pm z_{\\frac{\\alpha}{2}}(\\frac{\\sigma}{\\sqrt{n}})\\] Where: \\(\\bar{x}\\) is the sample mean \\(\\alpha\\) is the significance level \\(\\sigma\\) is the population standard deviation, which we know ex ante \\(n\\) is the sample size Like constructing all confidence intervals, we choose a level of \\(\\alpha\\) (Type I error) first, then build the interval. The confidence interval gives us a range around our sample estimate with a (1 - \\(\\alpha\\)) probability that it will capture the true population parameter. Key Takeaways: We build the confidence interval using the Z-distribution as opposed to T-distribution when the population is normal and variance is known For a given level of alpha, the interval built using the Z-distribution will be narrower than one built using the T-distribution The expression for confidence interval uses a rearrangement of the formula for the \\(Z\\) statistic. The \\(Z\\) statistic relates \\(\\bar{x}\\) with our hypothesized value for the population mean, \\(\\mu\\). When it comes to interpretation, from a frequentist standpoint, the confidence interval is not the probability that the population will fall into the range. Instead, it is the probability that our confidence interval captures the population parameter. That’s because what’s random is our sample, not the population parameter. 12.1.4 Hypothesis Testing The formula for the test statistic, in this case the Z-statistic, used for hypothesis testing is: \\[Z = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\] Where: \\(\\bar{x}\\) is the sample mean \\(\\mu\\) is the population mean \\(\\alpha\\) is the significance level \\(\\sigma\\) is the population standard deviation, which we know ex ante \\(n\\) is the sample size Z follows a normal distribution with mean of 0 and variance of 1. For hypothesis testing, we either use Z to determine a p-value. Or we see if Z exceeds a certain threshold value. Key Takeaways: In the below, “large” Z-statistics and Z-values is “large” in the absolute sense. Large Z-statistic values mean that the distance between our sample mean and hypothesized population mean value is large The p-value is the probability of seeing a Z-value at least as large as the observed Z-statistic. Large Z-statistics lead to small p-values. Under the p-value method, small p-values suggest a rejection of the null hypothesis as it is highly unlikely that given our sample mean, the population mean is at least as far away as our hypothesized value. We reject the null hypothesis if the p-value is less than \\(\\alpha\\). Usually, we reject the null hypothesis if the p-value is less than 0.05. Under the critical value method, large Z-statistics suggest a rejection of the null hypothesis. A large Z-statistic suggests that there is a low likelihood that the deviation between the hypothesized value and our sample statistic is that large. Thus, it implies that it is unlikely that the hypothesized value is correct. We reject the null hypothesis if the test statistic is larger in absolute value than the critical value. As a rule of thumb, Z-statistics larger than 2 suggests a rejection of the null hypothesis. 12.1.5 Suggested Steps Here are some suggested steps for performing mean inference in practice. Point Estimation: Make sure you know the population variance ex ante. This is unrealistic in practice so we generally do not use this inference method. Obtain sample data and ensure it is iid. Random samples are usually iid, by definition. Check that the sample data fits the normal assumption using methods such as QQ plot. If it is not normal, you cannot use this method. Calculate the sample mean Confidence Interval: Continuing from above… Choose an \\(\\alpha\\) Construct the confidence interval The confidence interval is a range that has a \\((1 - \\alpha)\\) percent chance of capturing the true population mean Hypothesis Testing: Continuing from above… Set up your \\(H_{0}\\) and \\(H_{a}\\) statements Calculate the Z-statistic Using either statistical software or the Z-Table, use either the p-value or critical value method to determine whether or not to reject your null hypothesis Interpret your results 12.1.6 Doing It In R Coming soon 12.1.7 Sources and Useful Links Sources: Degroot: Probability And Statistics (4th) - DeGroot, Schervish Useful Links: https://en.wikibooks.org/wiki/Statistics/Testing_Data/z-tests 12.2 Normal-Univariate-Frequentist-Unknown-Variance This section covers population mean inference when the population distribution is assumed to be normal and population variance is unknown. More specifically, mean inference is performed under the following assumptions and conditions: Assumptions: Population follows a normal distribution Observations are iid Unknown population variance Other Conditions, Criteria, or Attributes: Univariate mean inference Frequentist perspective 12.2.1 Overview and Summary Under the assumptions above, inference can be done using the T-statistic and T-distribution. Key Takeaways: We calculate the T-statistic and use the T-values and T-distribution to create the confidence interval and perform hypothesis testing since the population variance is unknown. If the population variance is known, we can use the Z-statistic and Z-distribution. If the sample size is greater than 30, our results from using the T-statistic and T-distribution will be very similar to results from using the Z-statistic and Z-distribution You can think of the test statistic as a way to relate the sample data, our hypothesized population parameter, and a likelihood. T can be plugged into the PDF of a T-distribution to return the likelihood of our sample statistic and our hypothesized value being the values provided. 12.2.2 Point Estimation Most commonly we use the following formula to generate an unbiased estimate of the population mean. The formula is: \\[\\bar{x} = \\frac{\\sum^n_{i=1}{x_{i}}}{n}\\] Where: \\(\\bar{x}\\) = unbiased estimate of population mean \\(n\\) = sample size \\(x_{i}\\) = your sample observations The formula above not only produces an unbiased estimate under the assumptions in this section, but always produces an unbiased estimate of the population mean no matter the distribution (as long as the population mean exists). Key Takeaways: The above formula will produce an unbiased estimate no matter what the population distribution is. While you could use maximum likelihood estimation, method of moments, or other estimation techniques to produce a point estimate for the population mean, in practice we just use the formula above. Under the normal assumption, both maximum likelihood estimation and method of moments will result in the same estimate of the mean as the formula above. 12.2.3 Confidence Interval The formula for the confidence interval given our assumptions is: \\[\\bar{x} \\pm t_{\\frac{\\alpha}{2}, n-1}(\\frac{s}{\\sqrt{n}})\\] Where: \\(\\bar{x}\\) is the sample mean \\(\\alpha\\) is the significance level \\(s\\) is the sample standard deviation using the corrected sample standard deviation formula \\(n\\) is the sample size \\(t\\) comes from a t-distribution with \\(n - 1\\) degrees of freedom Like constructing all confidence intervals, we choose a level of \\(\\alpha\\) (Type I error) first, then build the interval. The confidence interval gives us a range around our sample estimate with a (1 - \\(\\alpha\\)) probability that it will capture the true population parameter. Key Takeaways: We build the confidence interval using the T-distribution as opposed to Z-distribution when the population is normal and variance is unknown For a given level of alpha, the interval built using the T-distribution will be wider than one built using the Z-distribution The expression for confidence interval uses a rearrangement of the formula for the \\(T\\) statistic. The \\(T\\) statistic relates \\(\\bar{x}\\) with our hypothesized value for the population mean, \\(\\mu\\). When it comes to interpretation, from a frequentist standpoint, the confidence interval is not the probability that the population will fall into the range. Instead, it is the probability that our confidence interval captures the population parameter. That’s because what’s random is our sample, not the population parameter. 12.2.4 Hypothesis Testing The formula for the test statistic, in this case the T-statistic, used for hypothesis testing is: \\[T = \\frac{\\bar{x} - \\mu}{\\frac{S}{\\sqrt{n}}}\\] Where: \\(\\bar{x}\\) = unbiased estimate of population mean \\(n\\) = sample size \\(S\\) = sample standard deviation using the corrected sample standard deviation formula T follows a Student’s T-distribution with \\(n-1\\) degrees of freedom. For hypothesis testing, we either use T to determine a p-value. Or we see if T exceeds a certain threshold value. Key Takeaways: In the below, “large” T-statistics and T-values is “large” in the absolute sense. Large T-statistic values mean that the distance between our sample mean and hypothesized population mean value is large The p-value is the probability of seeing a T-value at least as large as the observed T-statistic. Large T-statistics lead to small p-values. Under the p-value method, small p-values suggest a rejection of the null hypothesis as it is highly unlikely that given our sample mean, the population mean is at least as far away as our hypothesized value. We reject the null hypothesis if the p-value is less than \\(\\alpha\\). Usually, we reject the null hypothesis if the p-value is less than 0.05. Under the critical value method, large T-statistics suggest a rejection of the null hypothesis. A large T-statistic suggests that there is a low likelihood that the deviation between the hypothesized value and our sample statistic is that large. Thus, it implies that it is unlikely that the hypothesized value is correct. We reject the null hypothesis if the test statistic is larger in absolute value than the critical value. As a rule of thumb, with a large enough sample size, T-statistics larger than 2 suggests a rejection of the null hypothesis. 12.2.5 Suggested Steps Here are some suggested steps for performing mean inference in practice. Point Estimation: Obtain sample data and ensure it is iid. Random samples are usually iid, by definition. Check that the sample data fits the normal assumption using methods such as QQ plot. Calculate the sample mean Confidence Interval: Continuing from above… Choose an \\(\\alpha\\) Construct the confidence interval The confidence interval is a range that has a \\((1 - \\alpha)\\) percent chance of capturing the true population mean Hypothesis Testing: Continuing from above… Set up your \\(H_{0}\\) and \\(H_{a}\\) statements Calculate the T-statistic Using either statistical software or the T-Table, use either the p-value or critical value method to determine whether or not to reject your null hypothesis Interpret your results 12.2.6 Doing It In R We can do T-tests in base R using t.test(). In this example, I simulate our observations from a normal distribution. x in the below is a vector of sample data. 12.2.7 Sources and Useful Links Sources: Degroot: Probability And Statistics (4th) - DeGroot, Schervish Useful Links: https://stats.stackexchange.com/questions/85804/choosing-between-z-test-and-t-test https://en.wikibooks.org/wiki/Statistics/Testing_Data/t-tests https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/t.test 12.3 General-Univariate-Frequentist This section covers population mean inference when we do not make any prior assumptions about the distribution of the population. More specifically, mean inference is performed under the following assumptions and conditions: Assumptions: Population can follow any shaped distribution with finite mean (not necessarily normal) Observations are iid Sample size must be large enough or one should use a non-parametric method Population variance can be known or unknown (and must be finite) Other Conditions, Criteria, or Attributes: Univariate mean inference Frequentist perspective 12.3.1 Overview and Summary Under the assumptions above, inference can be done using the T-statistic and T-distribution. Key Takeaways: With the shape of the population distribution unknown, we will always use the T-statistic and T-distribution to create the confidence interval. This is true regardless if we know the population variance or not. When your sample size exceeds 30 or so (need citation), the results from doing a T-test will be very similar to the results from a Z-test. This is because the T-distribution converges to a Z-distribution (standard normal distribution) as your sample size increases. The rule of thumb is 30, but in practice it will depend on the skewness of the population distribution. The more skewed your population is, the more samples we need. (need citation) make sure not confuse with CLT. With sample sizes much larger than 30, we could use the Z-statistic for hypothesis testing and confidence interval building but it’s usually safer to always use the T-statistic. 12.3.2 Point Estimation Most commonly we use the following formula to generate an unbiased estimate of the population mean. The formula is: \\[\\bar{x} = \\frac{\\sum^n_{i=1}{x_{i}}}{n}\\] Where: \\(\\bar{x}\\) = unbiased estimate of population mean \\(n\\) = sample size \\(x_{i}\\) = your sample observations The formula above not only produces an unbiased estimate under the assumptions in this section, but always produces an unbiased estimate of the population mean no matter the distribution (as long as the population mean exists). Key Takeaways: The above formula will produce an unbiased estimate no matter what the population distribution is. While you could use maximum likelihood estimation, method of moments, or other estimation techniques to produce a point estimate for the population mean, in practice we just use the formula above. Under the normal assumption, both maximum likelihood estimation and method of moments will result in the same estimate of the mean as the formula above. 12.3.3 Confidence Interval The formula for the confidence interval given our assumptions is: \\[\\bar{x} \\pm t_{\\frac{\\alpha}{2}, n-1}(\\frac{s}{\\sqrt{n}})\\] Where: \\(\\bar{x}\\) is the sample mean \\(\\alpha\\) is the significance level \\(s\\) is the sample standard deviation using the corrected sample standard deviation formula \\(n\\) is the sample size \\(t\\) comes from a t-distribution with \\(n - 1\\) degrees of freedom Like constructing all confidence intervals, we choose a level of \\(\\alpha\\) (Type I error) first, then build the interval. The confidence interval gives us a range around our sample estimate with a (1 - \\(\\alpha\\)) probability that it will capture the true population parameter. Key Takeaways: We always use the T-distribution to build a confidence interval in this situation. The expression for confidence interval uses a rearrangement of the formula for the \\(T\\) statistic. The \\(T\\) statistic relates \\(\\bar{x}\\) with our hypothesized value for the population mean, \\(\\mu\\). When it comes to interpretation, from a frequentist standpoint, the confidence interval is not the probability that the population will fall into the range. Instead, it is the probability that our confidence interval captures the population parameter. That’s because what’s random is our sample, not the population parameter. 12.3.4 Hypothesis Testing The formula for the test statistic, in this case the T-statistic, used for hypothesis testing is: \\[T = \\frac{\\bar{x} - \\mu}{\\frac{S}{\\sqrt{n}}}\\] Where: \\(\\bar{x}\\) = unbiased estimate of population mean \\(n\\) = sample size \\(S\\) = sample standard deviation using the corrected sample standard deviation formula T follows a Student’s T-distribution with \\(n-1\\) degrees of freedom. For hypothesis testing, we either use T to determine a p-value. Or we see if T exceeds a certain threshold value. Key Takeaways: In the below, “large” T-statistics and T-values is “large” in the absolute sense. Large T-statistic values mean that the distance between our sample mean and hypothesized population mean value is large The p-value is the probability of seeing a T-value at least as large as the observed T-statistic. Large T-statistics lead to small p-values. Under the p-value method, small p-values suggest a rejection of the null hypothesis as it is highly unlikely that given our sample mean, the population mean is at least as far away as our hypothesized value. We reject the null hypothesis if the p-value is less than \\(\\alpha\\). Usually, we reject the null hypothesis if the p-value is less than 0.05. Under the critical value method, large T-statistics suggest a rejection of the null hypothesis. A large T-statistic suggests that there is a low likelihood that the deviation between the hypothesized value and our sample statistic is that large. Thus, it implies that it is unlikely that the hypothesized value is correct. We reject the null hypothesis if the test statistic is larger in absolute value than the critical value. As a rule of thumb, with a large enough sample size, T-statistics larger than 2 suggests a rejection of the null hypothesis. 12.3.5 Suggested Steps Here are some suggested steps for performing mean inference in practice. Point Estimation: Obtain sample data and ensure it is iid. Random samples are usually iid, by definition. Check the sample data to see if it is skewed. More skewed data means you need a larger Calculate the sample mean Confidence Interval: Continuing from above… Choose an \\(\\alpha\\) Construct the confidence interval The confidence interval is a range that has a \\((1 - \\alpha)\\) percent chance of capturing the true population mean Hypothesis Testing: Continuing from above… Set up your \\(H_{0}\\) and \\(H_{a}\\) statements Calculate the T-statistic Using either statistical software or the T-Table, use either the p-value or critical value method to determine whether or not to reject your null hypothesis Interpret your results 12.3.6 Doing It In R We can do T-tests in base R using t.test(). In this example, I simulate our observations from a beta distribution with shape parameters \\(\\alpha = 1\\) and \\(\\beta = 5\\), resulting in a skewed distribution. We know that the true population mean of such a distribution is \\(\\frac{1}{(1 + 5)} \\approx 0.1667\\). \\(x\\) in the below is a vector of sample data from this beta distribution. In the example below, we build a confidence interval with \\(\\alpha = 0.05\\). We also do a two-sided T-test with \\(\\alpha = 0.05\\) and our hypothesis testing statements as: \\[H_o = 0.25\\] \\[H_a = \\neq 0.25\\] set.seed(7) # allows you to reproduce the results x = rbeta(30, 1, 5) # generate 30 observations from beta(1, 5) t.test(x, # vector of data alternative = &quot;two.sided&quot;, # one or two sided test mu = 0.25, # hypothesized population mean value conf.level = 0.95 # confidence level, (1 - alpha) ) | | One Sample t-test | | data: x | t = -2.3251, df = 29, p-value = 0.02727 | alternative hypothesis: true mean is not equal to 0.25 | 95 percent confidence interval: | 0.1117072 0.2411425 | sample estimates: | mean of x | 0.1764248 Confidence Interval: The output shows what our confidence interval and sample mean are. Our sample mean is \\(0.1764\\) and confidence interval ranges from \\(0.117\\) to \\(0.2411\\). This implies that there is a 95% chance that our confidence interval contains the true population mean. Hypothesis Testing: Using the critical value method, we see that our T-statistic is \\(-2.3252\\), which is larger in absolute value than the critical value of \\(2.045\\), meaning we reject the null hypothesis. This means it is unlikely that the true population mean is \\(0.25\\) given our sample mean of \\(0.1764\\). Using the p-value method, we see that our p-value is \\(0.0273\\), which is less than our \\(\\alpha\\) of \\(0.05\\). This also means we reject the null hypothesis. 12.3.7 Sources and Useful Links Sources: Degroot: Probability And Statistics (4th) - DeGroot, Schervish Useful Links: https://stats.stackexchange.com/questions/85804/choosing-between-z-test-and-t-test https://en.wikibooks.org/wiki/Statistics/Testing_Data/t-tests "],
["point-inference-mean-diff.html", "13 Mean Difference Inference 13.1 Normal-Two-Population-Frequentist-Independent-Equal-Variance", " 13 Mean Difference Inference This section covers making inferences on the difference between two population means. In other words, we are looking to compare the means of two populations - often seeing if they are different from each other. Each section here covers mean diffrence inference under different conditions and assumptions. If you are looking to compare the means of more than two populations or groups, view the section on ANOVA. The sections are: Mean Diffrence Inference (n.u.f): normal, iid, univariate, frequentist, known variance Mean Difference Inference (n.u.f.u): normal, iid, univariate, frequentist, unknown variance 13.1 Normal-Two-Population-Frequentist-Independent-Equal-Variance This section covers making inferences on the difference between two population means. The following assumptions and conditions are used: Assumptions: Both populations are normally distributed The variances of the two populations are EQUAL. This is also known as Observations are iid Population variance can be known or unknown (check if it can be finite or infinite) Other Conditions, Criteria, or Attributes: Univariate mean inference Frequentist perspective 13.1.1 Overview and Summary Key Takeaways: With the shape of the population distribution unknown, we will always use the T-statistic and T-distribution to create the confidence interval. This is true regardless if we know the population variance or not. When your sample size exceeds 30 or so (need citation), the results from doing a T-test will be very similar to the results from a Z-test. This is because the T-distribution converges to a Z-distribution (standard normal distribution) as your sample size increases. The rule of thumb is 30, but in practice it will depend on the skewness of the population distribution. The more skewed your population is, there more samples we need. (need citation) make sure not confuse with CLT. With sample sizes much larger than 30, we could use the Z-statistic for hypothesis testing and confidence interval building but it’s usually just easier to always use the T-statistic. Overview and Summary: Here we discuss how to perform univariate mean inference when the population can be any shaped distribution (with finite mean) and our observations are iid. Inference is done from a frequentist’s perspective. This is a common way to perform mean inference. 13.1.2 Point Estimation Key Takeaways: Under normal assumption, both maximum likelihood estimation and method of moments will result in the same estimate of the mean as the above formula. The above formula will produce an unbiased estimate no matter what the population distribution is. Proof. Point Estimation: The following formula can be used to calculate an unbiased estimate of the population mean using sample data: \\(\\bar{x} = \\frac{\\sum^n_{i=1}{x_{i}}}{n}\\) Where: \\(\\bar{x}\\) = unbiased estimate of population mean \\(n\\) = sample size \\(x_{i}\\) = your observations The formula above not only produces an unbiased estimate under the assumptions on this page, but always produces an unbiased estimate of the population mean no matter the distribution (as long as the population mean exists). 13.1.3 Confidence Interval With unknown population distribution, we construct a confidence interval using the T-distribution and T-table. The formula for the T-statistic is: \\(T = \\frac{\\bar{x} - \\mu}{\\frac{S}{\\sqrt{n}}}\\) Where: \\(\\bar{x}\\) = unbiased estimate of population mean \\(n\\) = sample size \\(S\\) = sample standard deviation using the following formula 13.1.4 Hypothesis Testing With unknown population distribution, we can perform hypothesis testing using the T-distribution, T-table, and T-statistic (T-score). 13.1.5 Suggested Steps Point Estimation: Obtain sample data and ensure it is iid. Random samples are usually iid, by definition. Calculate the sample mean using the following formula: \\(\\bar{x} = \\frac{\\sum^n_{i=1}{x_{i}}}{n}\\) Confidence Interval: Continuing from above, now calculate the confidence interval using: 13.1.6 Sources and Useful Links "],
["inferential-statistical-tests.html", "14 Overview", " 14 Overview "],
["computational.html", "15 Overview", " 15 Overview Part III of the book covers computational methods in statistics. Such include methods like bootstrapping and other approximation/simulation techniques. "]
]
